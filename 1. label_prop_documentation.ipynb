{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad1481f",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e08143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix,precision_score,\\\n",
    "recall_score,roc_auc_score,classification_report,fbeta_score,precision_recall_curve,roc_curve,log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8847ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore') #we don't wanna see that\n",
    "np.random.seed(1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing datasets:\n",
    "train = pd.read_csv('C:/Users/MBBLABS/Desktop/Python/1. Models/3. Project/Data/less_feature/train.csv',index_col='Unnamed: 0')\n",
    "test = pd.read_csv('C:/Users/MBBLABS/Desktop/Python/1. Models/3. Project/Data/less_feature/test.csv',index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead81d5",
   "metadata": {},
   "source": [
    "### Preprocessing: \n",
    "#### 1. Changing data type <br> 2. splitting data <br> 3. assigning label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data's 'y' is of float type - lets change it's type to integer\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8752e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the data\n",
    "display(train.head(2))\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546aab3",
   "metadata": {},
   "source": [
    "#### Assigning '-1' as label to the unlablled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ac41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['y'].isnull()]['y']\n",
    "train['y'] = train['y'].fillna(-1)\n",
    "train['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b380fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data type chaged to int32\n",
    "train['y'] = train['y'].astype('int32')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ce570",
   "metadata": {},
   "source": [
    "#### -- For training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating X,y:\n",
    "X = train.iloc[:,:-1] \n",
    "y = train.iloc[:,-1] #it's a mixure of all data\n",
    "\n",
    "#separating X,y with label\n",
    "X_lbl = train[train['y']!=-1].iloc[:,:-1]\n",
    "y_lbl = train[train['y']!=-1].iloc[:,-1]\n",
    "X_lbl.shape,y_lbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d3989",
   "metadata": {},
   "source": [
    "#### -- For test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.iloc[:,:-1] \n",
    "y_test = test.iloc[:,-1] \n",
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f8ea2",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataframe to store results\n",
    "index = ['Algorithm', 'ROC AUC']\n",
    "results = pd.DataFrame(columns=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression:\n",
    "logreg = LogisticRegression(random_state=1, class_weight='None')\n",
    "logreg.fit(X_lbl, y_lbl)\n",
    "results = results.append(pd.Series(['Logistic Regression', roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])], \n",
    "                                   index=index), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying result of logistic regression\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test data\n",
    "pred_test = logreg.predict(X_test)\n",
    "\n",
    "#Calculating and printing the f1 score \n",
    "f1_test = f1_score(y_test, pred_test)\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, pred_test),annot=True,fmt='d',cmap = 'Blues')\n",
    "plt.title('Confusion Matrix',size = 15)\n",
    "plt.xlabel('Predictions',size =15)\n",
    "plt.ylabel('True Values',size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668379a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred_test),precision_score(y_test,pred_test),recall_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697dde7",
   "metadata": {},
   "source": [
    "#### Classification Report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3e182",
   "metadata": {},
   "source": [
    "#### Threshold tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "               return (pos_probs >= threshold)\n",
    " \n",
    "y_prob = logreg.predict_proba(X_test)[:,1]\n",
    " \n",
    "thresholds = np.arange(0, 1, 0.001)\n",
    "scores = [f1_score(y_test, to_labels(y_prob, t)) for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "ix = np.argmax(scores)\n",
    "print('Threshold=%.3f, F1-Score=%.5f' % (thresholds[ix], scores[ix]))\n",
    "\n",
    "\n",
    "plt.plot(thresholds, scores)\n",
    "plt.title('F1-score vs Threshold ')\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('F1-score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11891e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = to_labels(y_prob,0.443)\n",
    "print(classification_report(y_test, y_pred_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20567259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision \n",
    "pr =  precision_score(y_test,y_pred_tuned)\n",
    "#recall\n",
    "re = recall_score(y_test,y_pred_tuned)\n",
    "#accuracy\n",
    "acc = accuracy_score(y_test,y_pred_tuned)\n",
    "\n",
    "pr,re,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ed532",
   "metadata": {},
   "source": [
    "## Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b313d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(params_list,kernel,metric_scores,y_test_temp, y_pred,y_test_temp_best, y_prob_best):\n",
    "    fig, ([a1,a2],[b1,b2], [c1,c2], [d1,d2]) = plt.subplots(4,2,figsize=(20,30))\n",
    "    \n",
    "    #plots: \n",
    "    #1. ROC-AUC \n",
    "    _ = a1.plot(params_list, metric_scores['ROC-AUC_Score'],color = 'steelblue')\n",
    "    a1.set_xlabel('Gamma')\n",
    "    a1.set_ylabel('ROC-AUC_Score')\n",
    "    a1.set_title('Label Propagation ROC AUC with ' + kernel.upper() + ' kernel')\n",
    "    #a1.savefig('RBF_ROC-AUC.png')\n",
    "\n",
    "    #2.Threshold\n",
    "    _ = a2.plot(params_list,metric_scores['Threshold'],color = 'steelblue')\n",
    "    a2.set_xlabel('Gamma')\n",
    "    a2.set_ylabel('Threshold')\n",
    "    a2.set_title('Gamma vs Threshold with ' + kernel.upper() + ' kernel')\n",
    "    #a1.savefig('RBF_Threshold.png')\n",
    "\n",
    "    #3.F1,Precision,Recall\n",
    "    _ = b1.plot(params_list,metric_scores['F1_score'],color = 'steelblue',label = 'F1_Score')\n",
    "    b1.set_xlabel('Gamma')\n",
    "    b1.set_ylabel('F1_score')\n",
    "    b1.set_title('Gamma vs F1_score with ' + kernel.upper() + ' kernel')\n",
    "    b1.legend(loc=\"lower right\")\n",
    "    #a1.savefig('RBF_F1_score.png')\n",
    "\n",
    "    #4.Accuracy\n",
    "    _ = b2.plot(params_list,metric_scores['Accuracy'],color = 'steelblue',label = 'Accuracy')\n",
    "    b2.set_xlabel('Gamma')\n",
    "    b2.set_ylabel('Accuracy')\n",
    "    b2.set_title('Gamma vs Accuracy with ' + kernel.upper() + ' kernel')\n",
    "    b2.legend(loc=\"lower right\")\n",
    "    #a1.savefig('RBF_Accuracy.png')\n",
    "\n",
    "    #5.Precision\n",
    "    _ = c1.plot(params_list,metric_scores['Precision'],color = 'steelblue',label = 'Precision')\n",
    "    c1.set_xlabel('Gamma')\n",
    "    c1.set_ylabel('Precision')\n",
    "    c1.set_title('Gamma vs Precision with ' + kernel.upper() + ' kernel')\n",
    "    c1.legend(loc=\"lower right\")\n",
    "    #a1.savefig('RBF_Precision.png')\n",
    "\n",
    "    #6.Recall\n",
    "    _ = c2.plot(params_list,metric_scores['Recall'],color = 'steelblue',label = 'Recall')\n",
    "    c2.set_xlabel('Gamma')\n",
    "    c2.set_ylabel('Recall')\n",
    "    c2.set_title('Gamma vs Recall with ' + kernel.upper() + ' kernel')\n",
    "    c2.legend(loc=\"lower right\")\n",
    "    #a1.savefig('RBF_Recall.png')\n",
    "\n",
    "    #Precision vs Recall - for BEST model:\n",
    "    precision, recall, _ = precision_recall_curve(y_test_temp_best, y_prob_best)\n",
    "    d1.step(recall, precision, color='steelblue',where='post')\n",
    "    d1.fill_between(recall, precision, step='post', color='lightgray')\n",
    "    d1.set_title('Precision-Recall Tradeoff')\n",
    "    d1.set_xlabel('Recall')\n",
    "    d1.set_ylabel('Precision')\n",
    "    #a1.savefig('RBF_Precison_Recall_for_best_model.png')\n",
    "    \n",
    "    #confusion matrix for best model:\n",
    "    sns.heatmap(confusion_matrix(y_test_temp, y_pred),annot=True,fmt='d',cmap = 'Blues')\n",
    "    d2.set_xlabel('True Values')\n",
    "    d2.set_ylabel('Prediction')\n",
    "    d2.set_title('Confusion Matrix')\n",
    "    #d2.savefig('RBF_confusion_matrix_for_best_model.png')\n",
    "    \n",
    "    plt.savefig(f'{kernel}_report.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0137017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prop_test(kernel, params_list, X_train, X_test, y_train, y_test,sampling):\n",
    "    n, g = 0, 0\n",
    "    \n",
    "    def to_labels(pos_probs, threshold):\n",
    "        return (pos_probs >= threshold)\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "      \n",
    "    \n",
    "    roc_scores = []\n",
    "    f1_sc = []\n",
    "    lst_report = []\n",
    "    data = []\n",
    "    f1_best = 0 \n",
    "    \n",
    "    if kernel == 'rbf':\n",
    "        for g in params_list:\n",
    "            lp = LabelPropagation(kernel=kernel, n_neighbors=n, gamma=g, max_iter=100000, tol=0.0001,n_jobs=-1)\n",
    "            lp.fit(X_train, y_train)\n",
    "\n",
    "            #roc_scores\n",
    "            lst = lp.predict_proba(X_test)[:,1]\n",
    "            temp = pd.Series(lst)\n",
    "            temp_lst = temp[temp.isnull()].index.tolist()\n",
    "            y_prob = np.delete(lst,temp_lst)\n",
    "            y_test_temp = np.delete(np.array(y_test),temp_lst)\n",
    "            rc = roc_auc_score(y_test_temp, y_prob)\n",
    "            \n",
    "            roc_scores.append(roc_auc_score(y_test_temp, y_prob))\n",
    "\n",
    "            #thresold tuning:\n",
    "            scores = [fbeta_score(y_test_temp, to_labels(y_prob, t),beta=1.5) for t in thresholds]\n",
    "            #get best threshold\n",
    "            ix = np.argmax(scores)\n",
    "            t_best = thresholds[ix] #best threshold ------------\n",
    "            y_pred = to_labels(y_prob, t_best)\n",
    "            scores[ix] #f1_score (beta)\n",
    "            \n",
    "            #f1_score calculation:\n",
    "            f1_test = f1_score(y_test_temp, y_pred)\n",
    "            f1_sc.append(f1_test)\n",
    "            \n",
    "            #storing the best performing model:\n",
    "            if f1_test >= f1_best:\n",
    "                f1_best = f1_test\n",
    "                lp_best = lp\n",
    "                y_prob_best = y_prob\n",
    "                y_test_temp_best = y_test_temp\n",
    "            \n",
    "\n",
    "            #classification report:\n",
    "            lst_report.append(classification_report(y_test_temp, y_pred))\n",
    "            \n",
    "            #precision \n",
    "            pr =  precision_score(y_test_temp,y_pred)\n",
    "            #recall\n",
    "            re = recall_score(y_test_temp,y_pred)\n",
    "            #accuracy\n",
    "            acc = accuracy_score(y_test_temp,y_pred)\n",
    "            \n",
    "            #log-loss:\n",
    "            lgloss = log_loss(y_test_temp,y_prob)\n",
    "            \n",
    "            #dataframe\n",
    "            data.append([g,lgloss,rc,t_best,f1_test,acc,pr,re])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #making a dataframe of scores:\n",
    "        metric_scores = pd.DataFrame(data,columns=['Gamma','Log_Loss','ROC-AUC_Score','Threshold','F1_score','Accuracy','Precision','Recall'])\n",
    "        \n",
    "        \n",
    "        #exporting metric_scores to csv\n",
    "        try:\n",
    "            with pd.ExcelWriter('6.V4_less_variable',engine='openpyxl',if_sheet_exists='replace',mode='a') as writer:\n",
    "                if sampling == 'Non_Treated':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_Non_Treated')\n",
    "                if sampling == 'undersampling':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_undersampling')\n",
    "                if sampling == 'oversampling':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_oversampling')\n",
    "                if sampling == 'smote':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_smote')\n",
    "        \n",
    "        except:\n",
    "            with pd.ExcelWriter('6.V4_less_variable.xlsx',engine='openpyxl') as writer:\n",
    "                if sampling == 'Non_Treated':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_Non_Treated')\n",
    "                if sampling == 'undersampling':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_undersampling')\n",
    "                if sampling == 'oversampling':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_oversampling')\n",
    "                if sampling == 'smote':\n",
    "                    metric_scores.to_excel(writer,engine='openpyxl', sheet_name='RBF_smote')\n",
    "        \n",
    "        \n",
    "        #ploting graphs:\n",
    "        plot_graphs(params_list,kernel,metric_scores,y_test_temp, y_pred,y_test_temp_best, y_prob_best)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "                       \n",
    "    if kernel == 'knn':\n",
    "        for n in params_list:\n",
    "            lp = LabelPropagation(kernel=kernel, n_neighbors=n, gamma=g, max_iter=100000, tol=0.0001,n_jobs=-1)\n",
    "            lp.fit(X_train, y_train)\n",
    "            \n",
    "            #roc_scores\n",
    "            lst = lp.predict_proba(X_test)[:,1]\n",
    "            temp = pd.Series(lst)\n",
    "            temp_lst = temp[temp.isnull()].index.tolist()\n",
    "            y_prob = np.delete(lst,temp_lst)\n",
    "            y_test_temp = np.delete(np.array(y_test),temp_lst)\n",
    "            roc_scores.append(roc_auc_score(y_test_temp, y_prob)) #---------\n",
    "            rc = roc_auc_score(y_test_temp, y_prob)\n",
    "            \n",
    "            \n",
    "            #thresold tuning:\n",
    "            scores = [fbeta_score(y_test_temp, to_labels(y_prob, t),beta=1.5) for t in thresholds]\n",
    "            # get best threshold\n",
    "            ix = np.argmax(scores)\n",
    "            t_best = thresholds[ix] #best threshold ------------\n",
    "            y_pred = to_labels(y_prob, t_best)\n",
    "            scores[ix] #f1_score (beta)\n",
    "            \n",
    "            #f1_score calculation:\n",
    "            f1_test = f1_score(y_test_temp, y_pred)\n",
    "            f1_sc.append(f1_test)\n",
    "            \n",
    "            #storing the best performing model:\n",
    "            if f1_test >= f1_best:\n",
    "                f1_best = f1_test\n",
    "                lp_best = lp\n",
    "                y_prob_best = y_prob\n",
    "                y_test_temp_best = y_test_temp\n",
    "            \n",
    "            \n",
    "            #classification report:\n",
    "            lst_report.append(classification_report(y_test_temp, y_pred))\n",
    "            #precision \n",
    "            pr =  precision_score(y_test_temp,y_pred)\n",
    "            #recall\n",
    "            re = recall_score(y_test_temp,y_pred)\n",
    "            #accuracy\n",
    "            acc = accuracy_score(y_test_temp,y_pred)\n",
    "            \n",
    "            #log-loss:\n",
    "            lgloss = log_loss(y_test_temp,y_prob)\n",
    "            \n",
    "            #dataframe\n",
    "            data.append([n,lgloss,rc,t_best,f1_test,acc,pr,re])\n",
    "            \n",
    "        metric_scores = pd.DataFrame(data,columns=['No_of_neighbors','Log_Loss','ROC-AUC_Score','Threshold','F1_score','Accuracy','Precision','Recall'])\n",
    "        display(metric_scores)\n",
    "        \n",
    "        \n",
    "        #ploting graphs:\n",
    "        plot_graphs(params_list,kernel,metric_scores,y_test_temp, y_pred,y_test_temp_best, y_prob_best)\n",
    "        \n",
    "        #exporting metric_scores to csv\n",
    "        with pd.ExcelWriter('6.V4_less_variable.xlsx',mode='a',engine='openpyxl',if_sheet_exists=\"replace\") as writer:\n",
    "            if sampling == 'Non_Treated':\n",
    "                metric_scores.to_excel(writer,engine='openpyxl', sheet_name='KNN_Non_Treated')\n",
    "            if sampling == 'undersampling':\n",
    "                metric_scores.to_excel(writer,engine='openpyxl', sheet_name='KNN_undersampling')\n",
    "            if sampling == 'oversampling':\n",
    "                metric_scores.to_excel(writer,engine='openpyxl', sheet_name='KNN_oversampling')\n",
    "            if sampling == 'smote':\n",
    "                metric_scores.to_excel(writer,engine='openpyxl', sheet_name='KNN_smote')\n",
    "\n",
    "\n",
    "    print('Best roc_score is {} at {}'.format(max(roc_scores),params_list[np.argmax(roc_scores)]))\n",
    "    print('-'*40)\n",
    "\n",
    "    print('Best f1_score is {} at {}'.format(max(f1_sc),params_list[np.argmax(f1_sc)]))\n",
    "    print('-'*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71055c62",
   "metadata": {},
   "source": [
    "## Without treating imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = [1e-5,1e-4,1e-3,1e-2,1e-1,2,4,5,8,10,12]\n",
    "label_prop_test('rbf',gamma, X, X_test , y, y_test,'Non_Treated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.arange(5,50)\n",
    "label_prop_test('knn',ns, X, X_test , y, y_test,'Non_Treated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df977b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [14]\n",
    "label_prop_test('knn',ns, X, X_test , y, y_test,'Non_Treated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250dc1c6",
   "metadata": {},
   "source": [
    "### Handling Imbalance of the data:\n",
    "### 1. Undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = train\n",
    "# Class count\n",
    "count_class_ulbl, count_class_0, count_class_1 = df2.y.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df2[df2['y'] == 0]\n",
    "df_class_1 = df2[df2['y'] == 1]\n",
    "df_class_ulbl = df2[df2['y'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0.shape,df_class_1.shape,df_class_ulbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6928bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0, count_class_1,count_class_ulbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample 0-class and concat the DataFrames of the classes:\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1,df_class_ulbl], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b11d71",
   "metadata": {},
   "source": [
    "#### Train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test_under.drop('y',axis='columns')\n",
    "y = df_test_under['y']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509e7ac0",
   "metadata": {},
   "source": [
    "#### Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.iloc[:,:-1] \n",
    "y_test = test.iloc[:,-1] \n",
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab77d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [1,2,3]\n",
    "label_prop_test('rbf', gammas, X, X_test, y, y_test,'undersampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414fdc3",
   "metadata": {},
   "source": [
    "### 2. Oversampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29498ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample 0-class and concat the DataFrames of the classes:\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace = True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over,df_class_ulbl], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_over.y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa220b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test_over.drop('y',axis='columns')\n",
    "y = df_test_over['y']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdd8c4",
   "metadata": {},
   "source": [
    "#### Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.iloc[:,:-1] \n",
    "y_test = test.iloc[:,-1] \n",
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20336b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [1]\n",
    "label_prop_test('rbf', gammas, X, X_test, y, y_test,'oversampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.arange(5,10)\n",
    "label_prop_test('knn', ns, X, X_test , y, y_test,'oversampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b51966",
   "metadata": {},
   "source": [
    "### 3. SMOTE:  Synthetic Minority Oversampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a572cac",
   "metadata": {},
   "source": [
    "Concatinating class_0 and class_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = pd.concat([df_class_0,df_class_1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sm.drop('y',axis='columns')\n",
    "y = df_sm['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_resample(X, y)\n",
    "\n",
    "y_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm1 = pd.concat([X_sm,y_sm],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smote = pd.concat([df_sm1,df_class_ulbl],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smote.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7cec0",
   "metadata": {},
   "source": [
    "#### Train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6778f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_smote.drop('y',axis='columns')\n",
    "y = df_smote['y']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf6d8d",
   "metadata": {},
   "source": [
    "#### Test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.iloc[:,:-1] \n",
    "y_test = test.iloc[:,-1] \n",
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dca49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [1]\n",
    "label_prop_test('rbf', gammas, X, X_test, y, y_test,'smote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.arange(5,50)\n",
    "label_prop_test('knn', ns, X, X_test , y, y_test,'smote')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
