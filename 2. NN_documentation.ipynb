{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Importing necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6314b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "train = pd.read_csv('C:/Users/MBBLABS/Desktop/Python/1. Models/3. Project/Data/less_feature/train.csv',index_col='Unnamed: 0')\n",
    "test = pd.read_csv('C:/Users/MBBLABS/Desktop/Python/1. Models/3. Project/Data/less_feature/test.csv',index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train.columns:\n",
    "    display(train[i].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['y'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268cc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train['y'].isnull()]['y']\n",
    "train['y'] = train['y'].fillna(-1)\n",
    "train['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data type chaged to int32\n",
    "train['y'] = train['y'].astype('int32')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a09b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating X,y with label:\n",
    "x_train_labeled = train[train['y']!=-1].iloc[:,:-1]\n",
    "y_train_labeled = train[train['y']!=-1].iloc[:,-1]\n",
    "\n",
    "x_train_labeled.shape,y_train_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating X,y with unlabel:\n",
    "x_train_unlabeled = train[train['y'] == -1].iloc[:,:-1]\n",
    "\n",
    "x_train_unlabeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data\n",
    "x_test = test.iloc[:,:-1] \n",
    "y_test = test.iloc[:,-1] \n",
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting distribution variables\n",
    "sns.set_style('darkgrid')\n",
    "features = x_train_labeled.columns\n",
    "\n",
    "plt.figure(figsize=(20,20*4))\n",
    "gs = gridspec.GridSpec(20, 4)\n",
    "for i, c in enumerate(x_train_labeled[features]):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.kdeplot(x=x_train_labeled[c][y_train_labeled==0],color='b',fill=True)\n",
    "    sns.kdeplot(x=x_train_labeled[c][y_train_labeled==1],color='r',fill=True)\n",
    "    plt.tight_layout\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6658db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Torch to work on Neural Network\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee415af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing all three datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_labeled = scaler.fit_transform(x_train_labeled)\n",
    "x_train_unlabeled = scaler.transform(x_train_unlabeled)\n",
    "x_test = scaler.transform(x_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting train dataset into tensor\n",
    "x_train_labeled = torch.from_numpy(x_train_labeled).type(torch.FloatTensor)\n",
    "y_train_labeled =y_train_labeled.to_numpy()\n",
    "y_train_labeled = torch.from_numpy(y_train_labeled).type(torch.LongTensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123200c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting test dataset into tensor\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_test=y_test.to_numpy()\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloaders for train & test sets\n",
    "train_labeled = torch.utils.data.TensorDataset(x_train_labeled, y_train_labeled)\n",
    "test = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting unlabeled dataset into tensor\n",
    "train_unlabeled = torch.from_numpy(x_train_unlabeled).type(torch.FloatTensor)\n",
    "# Creating the dataloader for unlabeled set\n",
    "unlabeled = torch.utils.data.TensorDataset(train_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c703e38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dataset is highly unbalaced,so Weighted Random Sampler is used for sampling to make each batch more balanced\n",
    "# Creating the Weighted Random Sampler for train set\n",
    "class_sample_count = np.array([len(np.where(y_train_labeled==t)[0]) for t in np.unique(y_train_labeled)]) # Counting number of points in each class\n",
    "print(class_sample_count)\n",
    "weight = 1. / class_sample_count # Weight is reciprocal of the sample count of each class\n",
    "print(weight)\n",
    "samples_weight = np.array([weight[t] for t in y_train_labeled]) # Assigning the weights to every point in the set\n",
    "print(samples_weight)\n",
    "samples_weight = torch.from_numpy(samples_weight) # Converting assigned weights to tensor\n",
    "print(samples_weight)\n",
    "sampler_labeled = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight)) #Creating the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ead58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataloaders for train, unlabeled and test sets.\n",
    "train_loader = torch.utils.data.DataLoader(train_labeled, batch_size = 64,sampler= sampler_labeled,shuffle = False, num_workers = 4)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(unlabeled, batch_size = 128,shuffle = False, num_workers = 4)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2662330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture of the Neural Network\n",
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(20, 16)\n",
    "            self.fc2 = nn.Linear(16, 18)\n",
    "            self.fc3 = nn.Linear(18, 20)\n",
    "            self.fc4 = nn.Linear(20, 24)\n",
    "            self.fc5 = nn.Linear(24, 2)\n",
    "            \n",
    "       \n",
    "        def forward(self, x):\n",
    "            \n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.dropout(x, p=0.25)\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = torch.sigmoid(self.fc5(x))\n",
    "            return x\n",
    "        \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights and bias\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38622f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss functtion\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea607c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Classification Report to evaluate the result\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32edb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to evaluate the network and get loss value and classification report on test set\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()     \n",
    "    loss = 0\n",
    "    pred1 = np.array([])\n",
    "    lbl1 = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data\n",
    "            output = model(data) # Generating the output                       \n",
    "            predicted = torch.max(output,1)[1] # Prdicting the class of the output                                                        \n",
    "            loss += loss_function(output,labels).item() # Updating loss with every epoch\n",
    "            \n",
    "            predicted1= predicted.cpu() \n",
    "            labels1= labels.cpu()\n",
    "            \n",
    "            pred = predicted1.detach().numpy()\n",
    "            lbl = labels1.detach().numpy()\n",
    "            \n",
    "            pred1 = np.append(pred1,pred)\n",
    "            lbl1 = np.append(lbl1,lbl)\n",
    "            \n",
    "    return (classification_report(lbl1,pred1)) , (loss/len(test_loader)),lbl1,pred1 # Getting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, training the model on the labeled set for 100 epochs\n",
    "from tqdm import tqdm_notebook\n",
    "def train_supervised(model, train_loader, test_loader):  # Defining a function to train the model on the labeled set\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # Using Adam optimizer with learning rate 0.001\n",
    "    EPOCHS = 50 # Number of epochs\n",
    "    model.train() # Training the model\n",
    "    for epoch in tqdm_notebook(range(EPOCHS)):\n",
    "        correct = 0\n",
    "        running_loss = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader): \n",
    "            X_batch, y_batch = X_batch, y_batch\n",
    "            \n",
    "            output = model(X_batch) # Generating the output\n",
    "            labeled_loss = loss_function(output, y_batch) # Calculating loss on labeled set\n",
    "            # Now, doing Backpropagation           \n",
    "            optimizer.zero_grad() # Resetting gradients\n",
    "            labeled_loss.backward() # Backward pass\n",
    "            optimizer.step() # Updating weights\n",
    "            running_loss += labeled_loss.item() # Updating loss with every epoch\n",
    "        \n",
    "        # Evaluating the model \n",
    "        report, test_loss =evaluate(model, test_loader)\n",
    "        print('\\n Epoch: {} | Train Loss : {:.7f} \\n Classification Report :\\n  {} \\n Test Loss : {:.7f} \\n'.format(epoch, running_loss/( len(train)), report, test_loss))\n",
    "        model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5242c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, running the funnction\n",
    "train_supervised(net, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b6df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the best result\n",
    "report, test_loss,lbl,pred = evaluate(net, test_loader)\n",
    "print('Classification Report : \\n  {} \\n Test Loss : {:.7f} '.format(report, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting the confusion matrix\n",
    "sns.heatmap(metrics.confusion_matrix(lbl, pred),annot=True,fmt='d',cmap = 'Blues')\n",
    "plt.title('Confusion Matrix',size = 15)\n",
    "plt.xlabel('Predictions',size =15)\n",
    "plt.ylabel('True Values',size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5508f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function of alpha_weight to control the contribution of unlabeled data to the overall loss. \n",
    "T1 = 15\n",
    "T2 = 30\n",
    "af = 3\n",
    "def alpha_weight(epoch):\n",
    "    if epoch < T1:\n",
    "        return 0.0\n",
    "    elif epoch > T2:\n",
    "        return af\n",
    "    else:\n",
    "         return ((epoch-T1) / (T2-T1))*af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f48ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, training the model on the labeled set and unalbeled data for 150 epochs\n",
    "import tqdm\n",
    "# Creating logs for alpha_weight, test_loss & classification report\n",
    "alpha_log = []\n",
    "test_loss_log = []\n",
    "report_log=[]\n",
    "\n",
    "def semisup_train(model, train_loader, unlabeled_loader, test_loader): # Defining a function to train the model on the labeled set & unlabeled set\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # Using Adam optimizer with learning rate 0.001\n",
    "    EPOCHS = 50 # Number of epochs    \n",
    "    # Instead of using current epoch we use a \"step\" variable to calculate alpha_weight to help the model to converge faster    \n",
    "    step = 10 # As for first 100 epochs , alpha_weight will be 0     \n",
    "    model.train()\n",
    "    for epoch in tqdm.notebook.tqdm(range(EPOCHS)):\n",
    "        for batch_idx, x_unlabeled in enumerate(unlabeled_loader):                       \n",
    "            # Forward Pass to get the pseudo labels\n",
    "            x_unlabeled = x_unlabeled[0]\n",
    "            model.eval()\n",
    "            output_unlabeled = model(x_unlabeled) # Generating the output for unlabeled set\n",
    "            _, pseudo_labeled = torch.max(output_unlabeled, 1) # Creating pseudo labels for unlabeled set\n",
    "            model.train() # Training the model\n",
    "            \n",
    "            # Now calculating the unlabeled loss using the pseudo label\n",
    "            output = model(x_unlabeled)\n",
    "            unlabeled_loss = alpha_weight(step) * loss_function(output, pseudo_labeled)   \n",
    "            \n",
    "            # Now, doing Backpropogation                        \n",
    "            optimizer.zero_grad() # Resetting gradients                        \n",
    "            unlabeled_loss.backward() # Backward pass                        \n",
    "            optimizer.step() # Updating weights\n",
    "                        \n",
    "            # Now, for every 50 batches training one epoch on labeled data \n",
    "            if batch_idx % 2 == 0:\n",
    "                \n",
    "                # Labeled data training \n",
    "                for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "                    X_batch = X_batch\n",
    "                    y_batch = y_batch\n",
    "                    output = model(X_batch) # Generating output of the labeled data\n",
    "                    labeled_loss = loss_function(output, y_batch) # Calculating loss of the labeled data\n",
    "                    # Again, doing Backpropagation\n",
    "                    optimizer.zero_grad()\n",
    "                    labeled_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Now step is increased by 1\n",
    "                step += 1\n",
    "                \n",
    "        # Evaluating the model\n",
    "        report, test_loss = evaluate(model, test_loader)\n",
    "        print('\\n Epoch: {} | Alpha Weight : {:.5f} | \\n Classification Report :\\n  {} \\n Test Loss : {:.7f} \\n'.format(epoch, alpha_weight(step), report, test_loss))\n",
    "        \n",
    "        # LOGGING VALUES \n",
    "        alpha_log.append(alpha_weight(step))\n",
    "        report_log.append(report)                \n",
    "        test_loss_log.append(test_loss)       \n",
    "        \n",
    "        model.train() # Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, running the funnction\n",
    "semisup_train(net, train_loader, unlabeled_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best result\n",
    "report, test_loss,_,_ = evaluate(net, test_loader)\n",
    "print('Classification Report : \\n {} | Test Loss : {:.7f} '.format(report, test_loss))\n",
    "\n",
    "# Saving the weights\n",
    "torch.save(net.state_dict(), 'Saved_models/Semi_supervised_weight_with_ST.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the saved weights\n",
    "net.load_state_dict(torch.load('Saved_models/Semi_supervised_weight_with_ST.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "report, test_loss,lbl1,pred1 = evaluate(net, test_loader)\n",
    "print('Classification Report : \\n {} | Test Loss : {:.7f} '.format(report, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting the confusion matrix\n",
    "sns.heatmap(metrics.confusion_matrix(lbl1, pred1),annot=True,fmt='d',cmap = 'Blues')\n",
    "plt.title('Confusion Matrix',size = 15)\n",
    "plt.xlabel('Predictions',size =15)\n",
    "plt.ylabel('True Values',size = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
